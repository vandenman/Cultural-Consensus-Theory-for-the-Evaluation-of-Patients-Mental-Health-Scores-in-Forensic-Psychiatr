In many psychiatric detention centers, patients’ mental health is monitored at regular intervals. Typically, clinicians score patients using a
Likert scale on multiple criteria including hostility. Having an overview of
patients’ scores benefits staff members in at least three ways. First, the
scores may help adjust treatment to the individual patient; second, the
change in scores over time allow an assessment of treatment effectiveness;
third, the scores may warn staff that particular patients are at high risk
of turning violent. Practical importance notwithstanding, current practices for the analysis of mental health scores are suboptimal: evaluations
from different clinicians are averaged (as if the Likert scale were linear
and the clinicians identical), and patients are analyzed in isolation (as if
they were independent). Uncertainty estimates of the resulting score are
often ignored. Here we outline a quantitative program for the analysis
of mental health scores using cultural consensus theory (CCT; Anders &
Batchelder, 2015). CCT models take into account the ordinal nature of
the Likert scale, the individual differences among clinicians, and the possible commonalities between patients. In a simulation, we compare the
predictive performance of the CCT model to the current practice of aggregating raw observations and, as a more reasonable alternative, against
often-used machine learning toolboxes. In addition, we outline the substantive conclusions afforded by application of the CCT model. We end
with recommendations for clinical practitioners who wish to apply CCT
in their own work.

Psychiatric detention centers monitor the mental health of their patients at
regular intervals, typically using a method such as Routine Outcome Monitoring (de Beurs et al., 2011). A clinician, psychiatrist, or another staff member,
henceforth a rater, scores a patient on multiple criteria. For example, a rater
evaluates a patient’s behavior on a variety of criteria that relate to aggressiveness. Today, such evaluations are stored so they may be used later to inform
decisions. The decisions informed by these ratings can vary widely. For instance, the scores may help adjust treatment to individual patients, the change
in scores over time allows for an assessment of treatment effectiveness, and the
scores may warn staff that particular patients are at high risk of turning violent.
Moreover, these ratings are key for a quantitative approach to monitoring and
forecasting patients’ behavior.
Current practices for aggregating the scores are suboptimal. Evaluations
from different raters are often averaged as if they are exchangeable. For example, personal communication with the staff of a psychiatric detention center
suggested that clinicians are more lenient in their ratings than psychiatrists,
but this information is not used to weigh their ratings. Furthermore, different
patients are analyzed in isolation, as if they are independent. Any background
information about patients, such as a patient’s criminal offense, is not accounted
for in a model-based manner. In addition, any uncertainty estimates of the resulting score are usually ignored.
Here, we try to address these issues using Cultural Consensus Theory (CCT;
Romney, Weller, & Batchelder, 1986; Batchelder & Romney, 1988; Batchelder &
Anders, 2012). The defining characteristic of CCT is that it aims to estimate the
consensus knowledge shared by raters. Hence, CCT is a promising framework
for analyzing data of psychiatric detention centers, where the true state of a
patient is unknown and needs to be estimated from the scores given by the
raters. CCT models capture individual differences between raters and items,
and pool information while accounting for these differences. However, currently
available CCT models can only be applied to the data of a single patient; a
limitation addressed in this paper.
The focus of this paper is to outline a quantitative program for the analysis of mental health scores using CCT. First, a CCT model for ordinal data
is introduced (Anders & Batchelder, 2015). Afterward, this model is expanded
step by step, to include more characteristics of the data, such as describing
multiple patients simultaneously. We showcase the model in three simulation
studies. First, we show that model parameters are retrieved accurately. Second,
we demonstrate how CCT could be used to monitor patients progress over time.
Third, we compare the predictive performance of the CCT model to the current
practice of aggregating raw observations and, as a more reasonable alternative,
against often-used machine learning toolboxes such as Random Forest (Breiman,
2001) and Boosted Regression Trees (Friedman, 2002). We showcase the substantive conclusions obtained from applying the CCT model and conclude the
paper with recommendations for clinical practitioners who wish to apply CCT
in their work.

The next sections introduce Cultural Consensus Theory (CCT). First, a brief
introduction to CCT is given. Afterward, the CCT model developed in Anders
and Batchelder (2015, henceforth, AB) is introduced, which serves as the simplest model for a single patient. Subsequently, we generalize the model in three
ways. First, the model is expanded to describe multiple patients simultaneously.
Next, latent constructs are added to the model. Finally, the model is adapted
to include background information on patients and raters.

Cultural Consensus Theory, also known as “test theory without an answer key”
(Batchelder & Romney, 1988), is a statistical tool that attempts to retrieve the
unknown “truth” for an item by examining the consensus among the responses.
For example, given a political questionnaire, there are no objectively correct
answers. Instead, one could administer the questionnaire to left-oriented respondents and use CCT to find out what the consensus is among left-oriented
respondents. CCT models capture that some responders have a higher competency and will strictly answer according to the cultural consensus. Likewise,
items can differ in their difficulty, i.e., the competence required to answer according to the consensus. For a political questionnaire, this implies that only
extremely left-oriented respondents agree with the most left-oriented political
statements. In addition, CCT models can be expanded to allow for multiple
consensus truths, i.e., there can be multiple unknown truths that vary across
subgroups of respondents (Anders & Batchelder, 2012). For a political questionnaire, the different consensuses (e.g., left, right, center, etc.) and respondents
membership to these groups would be estimated from the data. The property
of CCT models to estimate the consensus truth from the data is ideal for psychiatric data, where a patient’s true state is unknown and a consensus from the
raters is desired. CCT models can be applies to continuous data (e.g., the LTM;
Batchelder & Anders, 2012), binary data (e.g., the General Condorcet model;
Batchelder & Romney, 1986), and ordinal data (AB, 2015). Since ratings are
usually given on a Likert scale, we focus on a CCT model for ordinal data.

As a starting point, consider the Latent Truth Rater Model (LTRM), a cultural
consensus model for ordinal data introduced by AB. Figure 1 shows a graphical
model of the LTRM. The LTRM captures differences among raters and items
and may be viewed as the simplest model for a single patient. The rating given
by rater r on item i is denoted xri and takes on discrete values from 1 through
C. AB formalize the core ideas of the LTRM with 6 axioms, which are briefly
repeated here. There is a latent, shared cultural truth among the raters, which
is captured by the item location parameters θi (AB’s axiom 1). Since raters are
not perfect measurement instruments, they infer a noisy version of the cultural
truth for each item, called a latent appraisal and defined as yi = θi + ri , where
ri ∼ Logistic (0, ζr/κi ) (AB’s axiom 2). The appraisal error ri has variance
ζr/κi which varies across items and raters. This reflects that items fluctuate in
difficulty, captured by κi , and that raters vary in competence, captured by ζr
(AB’s axiom 3). Latent appraisals are assumed to be conditionally independent
given the latent truth θi and the appraisal error ri (AB’s axiom 4). So far,
the axioms describe a continuous latent process that underlies each observation.
To translate these continuous latent appraisals to categorical responses, it is
assumed that there exist C − 1 ordered thresholds δrc , such that each xri is
generated deterministically in the following way (AB’s axiom 5):


The appraisal yri is latent and thus we consider the probability that an appraisal
fails in between two thresholds to obtain the probability of an observed score.
This makes the generating process of xri probabilistic and described by an
ordered logistic distribution1 , which gives:

The thresholds δrc accommodate the response biases of the raters. AB do so by
estimating C − 1 ordered thresholds γ and defining δrc = αr γc + βr (AB’s axiom
6). This translation of thresholds is called the Linear in Log Odds function and
is a useful tool for capturing bias in probability estimation (Fox & Tversky,
1995; Gonzalez & Wu, 1999; Anders & Batchelder, 2015).
Figure 2 provides an intuition for how the ordered logistic distribution can
model different outcomes by varying only the rater parameters. The latent
appraisal y is fixed to 0, the thresholds γ are equal to logit (c/C) such that
P (xri | y = 0, γ, αr = 1, βr = 0) is uniform, and the scale αr and shift βr
vary. In the left panel, there is no response bias, αr = 1 and βr = 0, which
yields a uniform distribution over the predicted Likert scores. In the right panel,
an increase in response scale and shift, βr = .5 and αr = 2, concentrates the
predicted Likert scores around 2 and 3.

The LTRM is a complex model and unfortunately suffers from identification
issues, as was pointed out by AB already. For example, multiplying the rater
competences ζ and the item difficulties κ by a constant c yields an identical
variance for the appraisal distribution, since cζ/cκ = ζ/κ. Such identification
problems are avoided by restricting the mean of the respective parameters to 1
(as suggested in Appendix C in AB). Another identification problem originates
from estimating the thresholds individually. The number of thresholds, C − 1,
increases with the number of response options. This introduces a large number
of parameters that can be difficult to estimate, in particular when some response
options are not observed (i.e., when there are ceiling or floor effects). In addition,
the model is only identified if the sum of thresholds is zero ( c=1 γc = 0;
otherwise adding a constant to θi and δc yields an identical likelihood). Rather
than modeling each threshold individually, we describe the thresholds using only
two parameters per rater. Specifically, we model the thresholds as deviances
from an initial guess, γc = logit (c/C). This yields a set of thresholds such
that if the latent appraisal is 0 then P (xri ) is uniform. Response biases are
incorporated in the same manner: δrc = αr logit (c/C) + βr . This simplification
can still capture a wide variety of data sets (Selker, van den Bergh, Criss, &
Wagenmakers, 2019).

The LTRM as described above has many desired properties, for instance, it
captures individual differences among both raters and items. However, many
properties of psychiatric data are not captured by the model. three sections
generalize the LTRM to improve its capacity to describe the data at hand.

The first extension allows the model to describe multiple patients. Since different
patients can have different mental disorders the latent truth for an item varies
across patients to reflect this. Likewise, some items may be more difficult to
measure, but only for some patients. Both these changes can be achieved by
allowing the item truth θip and item difficulty κip to vary across patients. In
turn, this induces that the latent appraisal yrip varies across patients. As in
Figure 1, we assume that the patient parameters are drawn from a group-level
distribution with unknown mean and variance, for instance, the item difficulty
could follow a gamma distribution with unknown mean and variance (i.e., κip ∼
2
Gamma µκ/σκ2 , µκ/σκ2 ).
Extension II: Latent Constructs
Often, we are not just interested in the latent truth of a single item, but also
in a construct that is measured by multiple items. For instance, the latent
construct aggressiveness could be measured with multiple items. To accomplish
this, we introduce a latent variable ηpl and allow items to load on this latent
variable, i.e., we introduce a factor model over the items. The relation between
the latent construct and the item consensuses
is given by the regression weights

2
λil , such that θip ∼ Normal λil ηlp , σηp . The measurement model, i.e., which
items load on what construct, is assumed to be known.
As prior distribution on the latent constructs ηlp we used a normal distribution with variance 1, which reflects that the variance of a latent variable
is unidentified and restricted to 1. In addition, simulations showed that the
estimated regressions weights and the estimated patients’ scores on the latent
constructs exhibited label switching. For example, multiplying both the latent
constructs η and the regression weights λ by −1 yields the same distribution over
the item truths. To avoid label switching, we restricted the regression weights
to be positive, motivated from the perspective that it is typically known which
items are negatively scored (i.e., have a negative correlation with the latent
construct).
Extension III: Patient and Rater Information
The third extension adds background information about raters and patients to
the LTRM. This helps the model to capture that, for instance, child molesters

6

are typically less aggressive than murderers. Discrete patient and raters characteristics are captured by dividing the group-level distributions into separate
components for each category. For example, the hierarchical
distribution over

latent variables becomes ηpl ∼ Normal µCrimep , σCrimep , where the mean and
variance of the latent construct vary across patients that committed different
crimes. Rater characteristics are incorporated similarly, except that these would
influence the group-level distributions of rater-specific parameters. This yields
βr ∼ Normal (µStaff r , σStaff r ). For instance, this could capture that particular
groups of staff members may give more lenient ratings.
In the simulation studies, we restrict the analysis to discrete background
information. However, continuous background information could also be used.
Consider for instance the time a patient is committed to a detention center,
Timep . This information can be added as a regression on the mean of
 the grouplevel distribution. Thus, ηpl ∼ Normal µCrimep + ν Timep , σCrimep , where ν is
the regression coefficient from the time a patient is committed Timep on the
mean of the group-level distribution.
It is important to consider that the influence of background variables can
differ across latent constructs. For instance, the effect of a patient’s crime varies
across latent constructs, allowing the model to capture that child molesters and
murderers differ in aggression, but not on depression. This is accomplished by
estimating the effect of a patient’s crime separately for each latent construct,
which implies µl,Crimep .

Figure 3 graphically summarizes the extended LTRM. The extended LTRM
first separates the rater-specific influences from the data xrip , hereby accounting
for different groups of raters. This results in a latent consensus for each item
and patient θip . This consensus is subsequently used as an indicator for a
latent construct for all patients and constructs ηpl . The relation between the
latent construct and the items is given by the regression weights λil , such that
θip ∼ Normal (λil ηlp , 1). The factor scores also incorporate patient-specific
background information, such as the crime a patient committed.

The next sections illustrate the LTRM in a variety of scenarios. First, we demonstrate the benefit of the LTRM over the raw means in an example analysis of two
fictitious patients. Second, we demonstrate that the parameters of the LTRM
can be accurately recovered. Last, we compare the predictive performance of the
LTRM to the unweighted mean of the observations and two machine learning
toolboxes.
We estimate the parameters of the LTRM and the extended LTRM using a
Bayesian approach. Therefore, we are interested in the posterior distributions of
the model parameters. All models were written in Stan and approximated the
posterior distributions with variational inference (Carpenter et al., 2017). We
opted to use variational inference over traditional Markov chain Monte Carlo
because it was computationally fast while providing similar results in terms of
parameter retrieval and model predictions. All data was simulated using R (R
Core Team, 2019) and Stan models were run using the R package RStan (Stan
Development Team, 2019). R files and Stan models are available in the online
appendix at https://osf.io/jkv38/.

Here we showcase the benefits of a CCT analysis by examining results for two
fictitious participants. This example demonstrates how misleading the sample
mean can be. We simulated a data set of 50 patients, 10 raters, 20 items, and
5 answer categories. The items loaded on 3 different latent constructs, further
referred to as aggressiveness, anxiety, and depression. A patient-specific covariate, consisting of 5 categories was added to mimic the effect of a patient’s
criminal offense. Similarly, two different categories were used to imitate the
different groups of raters (e.g., clinicians and psychiatrists). Next, we selected
two patients whose differences in observed means were small relative to their
differences in posterior means on the latent constructs. The overall mean of the
observed ratings was 3.51 and 3.08 for patient 1 and 2 respectively. The means
for items of each construct are shown in Table 1. Analyzing these patients indeTable 1: Raw means of the observed ratings for the two patients with similar
mean responses yet different posterior distributions. The means are computed
for each latent construct and for all scores (overall).


Analyzing these patients independently by aggregating the raw observations indicates that these two patients
might differ in aggressiveness and depression but not in anxiety. However, after fitting the extended LTRM to the data it becomes apparent that there is
more to the data than what is shown by these averages. Using the extended
LTRM, we can visualize the posterior distributions of the latent constructs for
both patients, shown in Figure 4. The posterior distributions tell a different
story than Table 1. Remarkably, for construct 2 where the difference in means
is approximately equal, the posterior distributions differ. This difference can
be quantified by computing the probability that the posterior probability that
patient 1 has a larger value on a latent trait than patient 2. This probability is
approximated by counting how often the posterior samples of a latent construct
are larger for patient 1 than for patient 2. For all three constructs, the probability that patient 1 has a higher score is larger than 0.99 (Figure A.1 visualizes
these probabilities).
Altogether, this example shows that there is more information in the data
than what the averages say. If we examine the parameters of the data generating model more closely, we see that there are two reasons for this discrepancy.
The patients differ in item difficulty for Anxiety (1.42 for patient 1 and 0.88 for
patient 2) and crime committed, which means that the population level distributions differ. In this example, all raters rated both patients. In practice, the
ratings of different patients are likely given by different raters, which introduces
another source of bias. The discrepancy between the sample mean and posterior
mean is shown for all patients in Figure A.2, which further emphasizes that the
sample mean is an inadequate description of the patients’ scores.

A key step in developing a model is to assess if the model parameters can be
retrieved accurately. For this purpose, we simulated data as in the previous
example; the simulated data set consisted of 50 patients, 10 raters, 20 items,
and 5 answer categories. The items loaded on 3 different latent constructs.
A patient-specific covariate, consisting of 5 categories was added to mimic the
effect of a patient’s criminal offense. Similarly, two different categories of raters
were assumed. Figure 5 displays the true values against the posterior means for
each parameter.

All parameters are retrieved adequately. An exception is the item difficulty
κ which estimates appear more variable as the true item difficulty increases.
The spread in posterior means of the item difficulty is similar to that in Figure 6 in AB. The item truths θ appear seem underestimated as their magnitude
increases. It is likely that the hierarchical structure of the extented LTRM
shrinks the item truths towards the mean. Typically, there is more shrinkage if
the values of the parameter are larger, as is the case here. This bias does not
appear to influence the other parameter much, as the latent constructs η are
retrieved accurately.
Although it is good to know when the parameters of the extended LTRM can
be recovered, it may be more useful to know when the data are not informative
enough to use the extended LTRM. This is likely the case when there are few
items and raters. Exact numbers, however, may vary depending on the specific
situation at hand. For most purposes, it is straightforward to adjust the number
of raters, items, and patients, and then repeat the simulation. As an exercise,
we also recovered the parameters for AB’s LTRM. Code for the simulation is
available in the online appendix and parameter recovery is shown in Figure B.1.

Predictive Performance
Here, we compare the predictive performance of the LTRM to that of the sample
mode and, as a more informative comparison, to Random Forest and Boosted
Regression Trees (Boosting). Random Forest and Boosting analyses were done
using the R packages ranger and gbm respectively (Wright & Ziegler, 2017;
Greenwell, Boehmke, & Cunningham, 2019). We used the default settings for
the hyperparameters in both R packages.
We simulated two data sets each consisting of 20 raters, 30 items, 50 patients,
and thus in total 30,000 observations. The first data set represented a dense
design, where all raters scored all patients. The second data set represented
a sparse design, where each rater scored 10 patients. Raters were assigned to
patients so that the number of obtained scores was about equal for all patients.
To simulate a sparse data set, we first simulated a dense data set and subsequently removed a score if the raters that did not rate a patient. This remaining
sparse data set consisted of 6,000 observations. Next, both data sets were split
into a training set (80%) and a test set (20%). The performance of the four
methods was evaluated by training the models on the training set and using
the trained model to predict the outcomes for a test set. For the LTRM, we
used the mean of the posterior predictive distribution as a point-prediction.2
Predictions for Random Forest and Boosting were obtained by taking the majority vote of the trained classification trees. We used the observed mode of all
observations for the same rater, item, or patient (i.e., to predict xrip we used
mode(x−r,ip , xr,−i,p , xri,−p )).3
A technical difference between this approach and the previous simulations
is that model predictions are effectively treated as missing data. In terms of
implementation, a complete data set, where every patient is scored on each
item by all raters, can be represented as an array of dimensions R × I × P .
However, it is easier to handle an incomplete data set by representing the data
as a matrix of (R × I × P ) rows and 4 columns (i.e., long format).4 In this
format, the first column indicates the outcome, the second the rater, the third
the item, the fourth the patient, and subsequent columns indicate rater and
patient covariates.
We quantified prediction accuracy as the number of correct responses divided
by the total number of responses.

In this particular example, model predictions could also be interpreted as imputing missing
values. If these are regarded as missing observations rather than predictions, they should be
modeled as unknown discrete parameters of the model (Ch. 8; Gelman et al., 2014). That way,
uncertainty about these missing observations is propagated into the parameters. Although we
did not sample the missing observations from the joint posterior distribution, the code in the
online appendix does show how to do this.
3 We used the mode rather than the mean because the mean’s predictions lie outside of the
ordinal scale and thus prediction accuracy cannot be quantified in the same manner as for the
other methods.
4 For a Stan implementation the long format is even required as missing values must be
handled explicitly, unlike for other software e.g., JAGS.

Given that the data were generated by the LTRM, it comes as no surprise
that it predicts more accurately than the other methods. However, even though
data generated from the LTRM is likely a gross simplification of reality, the
results show that black-box machine learning methods perform somewhat adequately. This is somewhat surprising because the data at hand are ill-suited
for black-box machine learning methods, as these have difficulty capturing the
hierarchical structure of the data which contains most of the information (but
see Hajjem, Bellavance, & Larocque, 2014). Instead, if a lot of background information about patients and raters is available, this could likely improve their
performance. However, machine learning methods do not provide interpretable
models, which may be undesirable in practice because it makes it difficult to
substantiate decisions.

In this paper, we extended the Cultural Consensus model developed by Anders
and Batchelder (2015) to be suited for data often encountered in psychiatric
detention centers. The original model was suited for data from a single patient
and we extended this to multiple patients, latent constructs, and patient and
rater specific covariates. The benefit of this approach is that we can obtain
estimates for, for example, a patients aggressiveness while accounting for rater
bias, item-specific measurement error, and a patient’s criminal offense. We
have shown in a simulation that the parameters of the extended LTRM can be
retrieved accurately.
Although the LTRM provided better predictions than black-box machine
learning approaches, this is likely because the data were simulated from the
LTRM. It seems more reasonable that an optimal method for prediction would
combine results from the LTRM with some machine learning approach. For
example, augmenting a Random forest model with features based on psychological theories resulted in a model with better predictions of human decisions
than models based on psychological theories or naive machine learning models
(Plonsky et al., 2019; Plonsky, Erev, Hazan, & Tennenholtz, 2017). However,
machine learning approaches, despite their predictive power, result in uninterpretable models which may be undesirable in psychiatric practice where decisions need to be motivated and possibly defended (e.g., when determining
whether a treatment is effective or when deciding if a patient should be released).
Ideally, patients are monitored over some time and data from multiple measurement occasions is obtained. Next, the LTRM is applied to analyze the data.
Rather than applying the LTRM repeatedly to data from individual measurement occasions, all observations should be analyzed simultaneously. That way,
a patient’s progress could be monitored over time and predictions for the future time points could be obtained along with uncertainty estimates. To extend
the LTRM to incorporate time-varying components is straightforward, but the
exact properties of the time varying-components should depend on the data at
hand. For example, one can imagine that the factor scores of a patient vary
over time. A model to describe these changes would be a dynamic factor model
(Molenaar, 1985; Forni, Hallin, Lippi, & Reichlin, 2000). However, if patients
are only rated, say, every six months then a rigorous time series model is likely

unfit due to the small number of time points. Instead, simply estimating the
difference between consecutive time points with an intercept may suffice. For
these reasons we did not explore a time series extention of the LTRM.

In the LTRM, we assumed that the factor structure is known. In practice,
however, this need not be the case. Estimating the factor structure from the
data is possible, although such an endeavor shifts the focus of the LTRM to
model selection rather than assessing the progress of patients. Another more
flexible approach is to view the latent true scores of the items as a network and
estimate the relations among the items (but see Epskamp, Kruis, & Marsman,
2017 for possible drawbacks).
Since the posterior distributions were approximated with variational inference, it is possible that the obtained posterior distributions are biased. In
general, these biases rarely affect the estimated posterior means, but the posterior variance can be underestimated (Blei, Kucukelbir, & McAuliffe, 2017). As
a consequence, uncertainty estimates may be too narrow. To alleviate this, it is
relatively trivial to modify the Stan code in the appendix to use MCMC instead
of variational inference (e.g., in the code in the appendix change vb(model) to
sampling(model) to use MCMC). However, note that MCMC algorithms for
the models discussed run for hours to obtain a reasonable number of posterior
samples, whereas variational inference finishes after several minutes.

To successfully apply the extended LTRM in practice, the data should meet
several minimum requirements. Although the model accounts for differences
between raters, it is best to minimize these differences, for instance through
clear scoring instructions. In addition, there should be overlap among (groups
of) raters and the patients they score. That is, patients should be scored by
multiple raters in such a way that there are no isolated groups of raters and
patients, where one group of raters only rates one group of patients and another
group of raters only rates a different group of patients. A lack of overlap between
two groups complicates a comparison between raters and patients between two
groups. A lack of overlap can be avoided by having rater 1 score patients 1
through 5, having rater 2 score patients 3 through 7, etc. Additional information
about patients should be added to the model, such as the reason of incarceration.
That should help the extended LTRM to distinguish between groups of patients
that differ on these covariates. This also holds for the raters; if there are certain
background variables that are suspect to causing rater bias then these should
be included in the model.
To sum up, we extended the Latent Truth Rater model (LTRM) introduced
by Anders and Batchelder (2015) to a model that is suited for data typical
in psychiatric detention centers. The model accounts for individual differences
between raters, items, and patients. We demonstrated that the extended LTRM
can provide more information about the data at hand than the raw means for
two fictitious patients. In addition, we have shown that the parameters of the
extended LTRM can be adequately retrieved and that the LTRM outperforms
the observed mode and several machine learning toolboxes in terms of predictive
power. Finally, we have provided recommendations for clinical practitioners who
wish to apply the LTRM in practice. Altogether, we hope the extended LTRM
improves current practices for analyzing mental health scores in psychiatric
detention centers.
